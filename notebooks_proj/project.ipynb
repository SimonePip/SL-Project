{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_veeJ32BrfR"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zxRpUUr-BnaJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Lk7tXbB_Vf"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4TPaksx6CCw2"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_esteso.csv', header ='infer')#.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>titolo</th>\n",
       "      <th>durata</th>\n",
       "      <th>views</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>n_like</th>\n",
       "      <th>genere</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>publ</th>\n",
       "      <th>max_quality</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Kinect Sports - Tennis da tavolo Gameplay - Ev...</td>\n",
       "      <td>340</td>\n",
       "      <td>13328</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>110000</td>\n",
       "      <td>2010-11-11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.298205</td>\n",
       "      <td>0.218196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pro Skaters at Morrison Hill</td>\n",
       "      <td>210</td>\n",
       "      <td>5664</td>\n",
       "      <td>13</td>\n",
       "      <td>46</td>\n",
       "      <td>Sports</td>\n",
       "      <td>317</td>\n",
       "      <td>2009-07-20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.219331</td>\n",
       "      <td>0.167308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Yiruma-Shining Smile</td>\n",
       "      <td>205</td>\n",
       "      <td>11269</td>\n",
       "      <td>69</td>\n",
       "      <td>46</td>\n",
       "      <td>Music</td>\n",
       "      <td>431</td>\n",
       "      <td>2007-11-03</td>\n",
       "      <td>4</td>\n",
       "      <td>0.116417</td>\n",
       "      <td>0.093694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Guitarras de America (Portafolio)</td>\n",
       "      <td>294</td>\n",
       "      <td>3823</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Music</td>\n",
       "      <td>435</td>\n",
       "      <td>2009-07-15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.218508</td>\n",
       "      <td>0.137809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Review of Microsoft Band as a fitness watch</td>\n",
       "      <td>216</td>\n",
       "      <td>5464</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>Science &amp; Technology</td>\n",
       "      <td>20200</td>\n",
       "      <td>2014-11-06</td>\n",
       "      <td>6</td>\n",
       "      <td>0.537955</td>\n",
       "      <td>0.397559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8937</th>\n",
       "      <td>9122</td>\n",
       "      <td>Rob Ruggiero #7 Bridgewater State Football Hig...</td>\n",
       "      <td>210</td>\n",
       "      <td>4336</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>4</td>\n",
       "      <td>0.488062</td>\n",
       "      <td>0.318213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8938</th>\n",
       "      <td>9123</td>\n",
       "      <td>Rąbanie drewna w oponie</td>\n",
       "      <td>211</td>\n",
       "      <td>271266</td>\n",
       "      <td>42</td>\n",
       "      <td>223</td>\n",
       "      <td>Nonprofits &amp; Activism</td>\n",
       "      <td>7800</td>\n",
       "      <td>2012-03-17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.379219</td>\n",
       "      <td>0.415023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8939</th>\n",
       "      <td>9124</td>\n",
       "      <td>Open Heart four leaf clover nail design for Sa...</td>\n",
       "      <td>383</td>\n",
       "      <td>2852</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>Howto &amp; Style</td>\n",
       "      <td>36700</td>\n",
       "      <td>2013-03-11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.438334</td>\n",
       "      <td>0.314836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8940</th>\n",
       "      <td>9125</td>\n",
       "      <td>Campeonato Brasileiro Série C 2013: Treze 1x0 ...</td>\n",
       "      <td>361</td>\n",
       "      <td>4043</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>Sports</td>\n",
       "      <td>4140</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.474066</td>\n",
       "      <td>0.349952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8941</th>\n",
       "      <td>9126</td>\n",
       "      <td>WWE 2K14 WWE Universe Mode Part 498 Walkthrough</td>\n",
       "      <td>364</td>\n",
       "      <td>1817</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>30600</td>\n",
       "      <td>2014-06-17</td>\n",
       "      <td>5</td>\n",
       "      <td>0.514573</td>\n",
       "      <td>0.313898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8942 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             titolo  durata  \\\n",
       "0              0  Kinect Sports - Tennis da tavolo Gameplay - Ev...     340   \n",
       "1              1                       Pro Skaters at Morrison Hill     210   \n",
       "2              2                               Yiruma-Shining Smile     205   \n",
       "3              3                  Guitarras de America (Portafolio)     294   \n",
       "4              4        Review of Microsoft Band as a fitness watch     216   \n",
       "...          ...                                                ...     ...   \n",
       "8937        9122  Rob Ruggiero #7 Bridgewater State Football Hig...     210   \n",
       "8938        9123                            Rąbanie drewna w oponie     211   \n",
       "8939        9124  Open Heart four leaf clover nail design for Sa...     383   \n",
       "8940        9125  Campeonato Brasileiro Série C 2013: Treze 1x0 ...     361   \n",
       "8941        9126    WWE 2K14 WWE Universe Mode Part 498 Walkthrough     364   \n",
       "\n",
       "       views  n_comments  n_like                  genere  subscribers  \\\n",
       "0      13328          12      11                  Gaming       110000   \n",
       "1       5664          13      46                  Sports          317   \n",
       "2      11269          69      46                   Music          431   \n",
       "3       3823           6       7                   Music          435   \n",
       "4       5464           4      32    Science & Technology        20200   \n",
       "...      ...         ...     ...                     ...          ...   \n",
       "8937    4336           3       9          People & Blogs            0   \n",
       "8938  271266          42     223   Nonprofits & Activism         7800   \n",
       "8939    2852           9      44           Howto & Style        36700   \n",
       "8940    4043           6      43                  Sports         4140   \n",
       "8941    1817           0      11                  Gaming        30600   \n",
       "\n",
       "            publ  max_quality  timedelta     score  \n",
       "0     2010-11-11            4   0.298205  0.218196  \n",
       "1     2009-07-20            4   0.219331  0.167308  \n",
       "2     2007-11-03            4   0.116417  0.093694  \n",
       "3     2009-07-15            3   0.218508  0.137809  \n",
       "4     2014-11-06            6   0.537955  0.397559  \n",
       "...          ...          ...        ...       ...  \n",
       "8937  2014-01-07            4   0.488062  0.318213  \n",
       "8938  2012-03-17            3   0.379219  0.415023  \n",
       "8939  2013-03-11            4   0.438334  0.314836  \n",
       "8940  2013-10-14            4   0.474066  0.349952  \n",
       "8941  2014-06-17            5   0.514573  0.313898  \n",
       "\n",
       "[8942 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles elaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/simone/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    string=tokenizer.tokenize(string)\n",
    "    for word in range(len(string)):\n",
    "        string[word] = string[word].lower() \n",
    "    #Remove stopwords\n",
    "    string = [word for word in string if not word in stopwords.words()]\n",
    "    #STEMMING\n",
    "    stemmer = PorterStemmer()\n",
    "    string = [stemmer.stem(word) for word in string]\n",
    "    return \" \".join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4433it [01:24, 52.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#remove all the bad stuff we don't need\n",
    "titles=list(data['titolo'])\n",
    "for i,title in tqdm(enumerate(titles)):\n",
    "    titles[i]=clean(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of different words is:  13015\n",
      "the total number of words is:  25322\n"
     ]
    }
   ],
   "source": [
    "#create dictionary with key the word and value the number of times it has been seen\n",
    "diz={}\n",
    "for title in titles:\n",
    "    for word in title.split(' '):\n",
    "        if word in diz:\n",
    "            diz[word]+=1\n",
    "        else:\n",
    "            diz[word]=1  \n",
    "print('the total number of different words is: ',len(diz.keys()))\n",
    "print('the total number of words is: ',np.sum(list(diz.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update value of the dictionary with its prob computed as #(times is observed)/#(total words)\n",
    "tot=np.sum(list(diz.values()))\n",
    "for key in diz.keys():\n",
    "    diz[key]=diz[key]/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 4433/4433 [00:00<00:00, 537442.18it/s]\n"
     ]
    }
   ],
   "source": [
    "prob_title=[]\n",
    "for title in tqdm(titles):\n",
    "    somma=0\n",
    "    for word in title.split(' '):\n",
    "        somma+=(diz[word])\n",
    "    prob_title.append(somma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns = ['titolo'])\n",
    "data['titolo']=prob_title\n",
    "data['titolo']=(data['titolo']-data['titolo'].min())/(data['titolo'].max()-data['titolo'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SjeiTzLDz3J"
   },
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk6Vu9B8FKUC"
   },
   "source": [
    "### Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zPpV61yRFRd2"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAXqkuGSRo6C",
    "outputId": "a43f65dd-42dd-4825-810a-8a25c90118cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['durata', 'views', 'n_comments', 'n_like', 'genere', 'publ',\n",
       "       'max_quality', 'score', 'timedelta', 'class', 'titolo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Vh8Mxd3IFn3-"
   },
   "outputs": [],
   "source": [
    "Y = data['class'] # Extract the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TjwRGC1LF_yM"
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['class','score','publ']) # Remove from the data useful to the analysis: \"score, publ, titolo, class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4kcZ6zbkM71k"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X['genere'] = le.fit_transform(X['genere']) # Transform the Categorical genere feature in a numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5xfQwZB6GN5C"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y, train_size=.8, random_state=42) # Split dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NnylF4nyHP05"
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train, train_size = .8, random_state = 42) # Split train in train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McL-rf4EIETM"
   },
   "source": [
    "### Excursus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfJ_pNuoIggO"
   },
   "source": [
    "We want to predict the class of a given video. The classes were defined using a home-made score.\n",
    "\n",
    "To predict we will try different models.\n",
    "Let's start with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sihcOc9uI77Y",
    "tags": []
   },
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Nf67Y6igJRo7"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxSP1WkcJzIn"
   },
   "source": [
    "REMINDER OF THE PARAMETERS\n",
    "\n",
    "\n",
    "C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOwe20T2JYWN"
   },
   "source": [
    "Try to use SVC without scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT1V1xqOMskK",
    "outputId": "d81d3c1e-2765-47f5-ea02-3ba82ae68a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16061185468451242\n",
      "0.4732394366197183 0.25\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma = 'auto', random_state = 42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbBrjFoJUAIv"
   },
   "source": [
    "Try to use SVC with Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOLK38YuUIO0",
    "outputId": "5e42c141-5d75-487d-cf12-62154784483d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8001545157307074\n",
      "0.8183098591549296 0.7949318076745261\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKoLo8RcUuK8"
   },
   "source": [
    "Try to use SVC with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6R5IQpLSU1f0",
    "outputId": "618acf6f-65ca-457b-a5fe-f0450fc43bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5908952871236659\n",
      "0.7394366197183099 0.5721278317152104\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4nb9eD-VBc_"
   },
   "source": [
    "Try to use SVC with MinMaxScaler and Standard Scaler (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-zC-ZjtVKW0",
    "outputId": "fb50c2aa-3bc6-4266-b11c-d5bc2b1c4250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8001545157307074\n",
      "0.8183098591549296 0.7949318076745261\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(),StandardScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSCZ8ep5VZmm"
   },
   "source": [
    "Try to use SVC with Standard Scaler and MinMaxScaler (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXpKohSrVhH_",
    "outputId": "b1cc0eb0-10ed-4ebd-e7be-08217907a407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5908952871236659\n",
      "0.7394366197183099 0.5721278317152104\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),MinMaxScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JzDW9SRVpeJ"
   },
   "source": [
    "Same Results as One Scaler.\n",
    "\n",
    "\n",
    "Best Results without Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUL7jdepWgXj",
    "outputId": "8b1e9ee6-3af7-487f-f941-0437878d52a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': array([  1.  ,  25.75,  50.5 ,  75.25, 100.  ]),\n",
       "                         'kernel': ('poly', 'rbf', 'sigmoid')},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "\n",
    "#def gridsearch\n",
    "params = {\n",
    "    'kernel' : ('poly', 'rbf', 'sigmoid'),\n",
    "    'C' : np.linspace(1, 100, num=5), \n",
    "    #'degree' : [3,5,8],\n",
    "    #'gamma' : ('auto','scale')\n",
    "    \n",
    "}\n",
    "\n",
    "search = GridSearchCV(svc,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1, refit=True, verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5rYVhLC7aiLn"
   },
   "outputs": [],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UUMPjA_rawaJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31184183717669406"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGUfbcicdG2b",
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "w9jsJjJCczXZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score using Random Forest without scaling data:  0.9276672522324696\n",
      "0.9352112676056338 0.9225135806749885\n"
     ]
    }
   ],
   "source": [
    "clf = RFC(random_state = 42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using Random Forest without scaling data: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "mKM5dKSUc9lt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9301919059217758\n",
      "0.9380281690140845 0.9245737979657882\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), RFC(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mkR3VGIcdATZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9266516527250583\n",
      "0.9338028169014084 0.9208304438280166\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), RFC(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "SSoHJ33mdEgY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier(n_jobs=-1,\n",
       "                                                               random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'randomforestclassifier__criterion': ['gini',\n",
       "                                                               'entropy'],\n",
       "                         'randomforestclassifier__max_features': [None],\n",
       "                         'randomforestclassifier__n_estimators': [50, 100,\n",
       "                                                                  200]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), RFC(random_state = 42, n_jobs = -1))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'randomforestclassifier__criterion' : ['gini','entropy'],\n",
    "          'randomforestclassifier__max_features' : [None],\n",
    "          'randomforestclassifier__n_estimators' : [50,100,200]\n",
    "         }\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9478873239436619 0.9497457235321314\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ghV7IJqudY3X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9506803687468519"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGUfbcicdG2b",
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w9jsJjJCczXZ"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost without scaling data:  0.9603909262916284\n",
      "0.9605633802816902 0.9599341192787796\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(random_state=42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost without scaling data: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "mKM5dKSUc9lt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost using MinMaxScaler:  0.9603909262916284\n",
      "0.9605633802816902 0.9599341192787796\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), XGBClassifier(random_state=42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost using MinMaxScaler: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "mkR3VGIcdATZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost using StandardScaler:  0.9603909262916284\n",
      "0.9605633802816902 0.9599341192787796\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), XGBClassifier(random_state=42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost using StandardScaler: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "SSoHJ33mdEgY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                       ('xgbclassifier',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      importance_type=None,\n",
       "                                                      interaction_constraints=None,\n",
       "                                                      learning_rate=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,...\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=42,\n",
       "                                                      reg_alpha=None,\n",
       "                                                      reg_lambda=None,\n",
       "                                                      scale_pos_weight=None,\n",
       "                                                      subsample=None,\n",
       "                                                      tree_method=None,\n",
       "                                                      validate_parameters=None,\n",
       "                                                      verbosity=None))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgbclassifier__learning_rate': [0.2, 0.3, 0.4],\n",
       "                         'xgbclassifier__n_estimators': [600, 800, 1000]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), XGBClassifier(random_state=42))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'xgbclassifier__learning_rate' : [.2,.3,.4],\n",
    "          'xgbclassifier__n_estimators' : [600,800,1000]\n",
    "         }\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgbclassifier__learning_rate': 0.3, 'xgbclassifier__n_estimators': 600}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params=search.best_params_\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[198,   8,   0,   0],\n",
       "       [  6, 324,   6,   0],\n",
       "       [  0,   6, 133,   1],\n",
       "       [  0,   0,   1,  27]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9605633802816902 0.9609757859454462 0.9607088856336363\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs Rest Classifier - ORCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier as ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:15] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:15] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:15] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                              ('xgbclassifier',\n",
       "                                               XGBClassifier(base_score=None,\n",
       "                                                             booster=None,\n",
       "                                                             colsample_bylevel=None,\n",
       "                                                             colsample_bynode=None,\n",
       "                                                             colsample_bytree=None,\n",
       "                                                             enable_categorical=False,\n",
       "                                                             gamma=None,\n",
       "                                                             gpu_id=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.3,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=None,\n",
       "                                                             min_child_weight=None,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             n_estimators=600,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             predictor=None,\n",
       "                                                             random_state=42,\n",
       "                                                             reg_alpha=None,\n",
       "                                                             reg_lambda=None,\n",
       "                                                             scale_pos_weight=None,\n",
       "                                                             subsample=None,\n",
       "                                                             tree_method=None,\n",
       "                                                             validate_parameters=None,\n",
       "                                                             verbosity=None))]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(MinMaxScaler(), XGBClassifier(learning_rate=xgb_params['xgbclassifier__learning_rate'],n_estimators=xgb_params['xgbclassifier__n_estimators'],random_state=42))\n",
    "classifier=ORC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9690140845070423 0.9674208275543228 0.9714435290974552\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs One Classifier - ORCO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier as OOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:40:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:40:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                             ('xgbclassifier',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_constraints=None,\n",
       "                                                            learning_rate=0.3,\n",
       "                                                            max_delta_step=None,\n",
       "                                                            max_depth=None,\n",
       "                                                            min_child_weight=None,\n",
       "                                                            missing=nan,\n",
       "                                                            monotone_constraints=None,\n",
       "                                                            n_estimators=600,\n",
       "                                                            n_jobs=None,\n",
       "                                                            num_parallel_tree=None,\n",
       "                                                            predictor=None,\n",
       "                                                            random_state=42,\n",
       "                                                            reg_alpha=None,\n",
       "                                                            reg_lambda=None,\n",
       "                                                            scale_pos_weight=None,\n",
       "                                                            subsample=None,\n",
       "                                                            tree_method=None,\n",
       "                                                            validate_parameters=None,\n",
       "                                                            verbosity=None))]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(MinMaxScaler(), XGBClassifier(learning_rate=xgb_params['xgbclassifier__learning_rate'],n_estimators=xgb_params['xgbclassifier__n_estimators'],random_state=42))\n",
    "classifier=OOC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9591549295774648 0.9684162621359224 0.9676597958895383\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.719891418878267\n",
      "PRECISION: 0.7567042708442585\n",
      "RECALL: 0.6952525427646787\n",
      "ACCURACY: 0.7859154929577464\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=42, multi_class = 'multinomial',\n",
    "                                                         solver = 'newton-cg',\n",
    "                                                         n_jobs = -1))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall_fscore_support(y_val, y_pred, average='macro')[0])\n",
    "print('RECALL:',precision_recall_fscore_support(y_val, y_pred, average='macro')[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.6966332868104037\n",
      "PRECISION: 0.7329750726280289\n",
      "RECALL: 0.6708911234396671\n",
      "ACCURACY: 0.7366197183098592\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),KNeighborsClassifier(n_jobs = -1) )\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall_fscore_support(y_val, y_pred, average='macro')[0])\n",
    "print('RECALL:',precision_recall_fscore_support(y_val, y_pred, average='macro')[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.9167199780371165\n",
      "PRECISION: 0.9035016321077023\n",
      "RECALL: 0.9320475034674064\n",
      "ACCURACY: 0.9140845070422535\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),DecisionTreeClassifier(random_state = 42) )\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall_fscore_support(y_val, y_pred, average='macro')[0])\n",
    "print('RECALL:',precision_recall_fscore_support(y_val, y_pred, average='macro')[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.9713753646704398\n",
      "PRECISION: 0.9721155792929519\n",
      "RECALL: 0.9706484049930653\n",
      "ACCURACY: 0.9633802816901409\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "precision_recall = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall[0])\n",
    "print('RECALL:',precision_recall[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.9701598836087464\n",
      "PRECISION: 0.9695111550374709\n",
      "RECALL: 0.9708434466019418\n",
      "ACCURACY: 0.9619718309859155\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), GradientBoostingClassifier(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "precision_recall = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall[0])\n",
    "print('RECALL:',precision_recall[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SSoHJ33mdEgY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('gradientboostingclassifier',\n",
       "                                        GradientBoostingClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'gradientboostingclassifier__learning_rate': [0.2,\n",
       "                                                                       0.225,\n",
       "                                                                       0.25],\n",
       "                         'gradientboostingclassifier__max_depth': [3, 4, 5],\n",
       "                         'gradientboostingclassifier__n_estimators': [70, 75,\n",
       "                                                                      80]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state = 42))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'gradientboostingclassifier__learning_rate' : [.2,.225,.25],#.285\n",
    "          'gradientboostingclassifier__n_estimators' : [70,75,80],\n",
    "          #'gradientboostingclassifier__n_iter_no_change':[2,5,10],\n",
    "          'gradientboostingclassifier__max_depth':[3,4,5]\n",
    "         }\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=1, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingclassifier__learning_rate': 0.25,\n",
       " 'gradientboostingclassifier__max_depth': 3,\n",
       " 'gradientboostingclassifier__n_estimators': 75}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_params=search.best_params_\n",
    "gb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200,   6,   0,   0],\n",
       "       [  9, 321,   6,   0],\n",
       "       [  0,   6, 134,   0],\n",
       "       [  0,   0,   0,  28]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967605633802817 0.9743917591308369 0.9748965046589191\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs Rest Classifier (GB) - ORCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=Pipeline(steps=[('standardscaler',\n",
       "                                               StandardScaler()),\n",
       "                                              ('gradientboostingclassifier',\n",
       "                                               GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                                          n_estimators=75,\n",
       "                                                                          random_state=42))]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), GradientBoostingClassifier(learning_rate=gb_params['gradientboostingclassifier__learning_rate'],n_estimators=gb_params['gradientboostingclassifier__n_estimators'],max_depth=gb_params['gradientboostingclassifier__max_depth'],random_state=42))\n",
    "classifier=ORC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619718309859155 0.9703739019879797 0.9698794055513708\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs One Classifier (GB) - ORCO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=Pipeline(steps=[('standardscaler',\n",
       "                                              StandardScaler()),\n",
       "                                             ('gradientboostingclassifier',\n",
       "                                              GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                                         n_estimators=75,\n",
       "                                                                         random_state=42))]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), GradientBoostingClassifier(learning_rate=gb_params['gradientboostingclassifier__learning_rate'],n_estimators=gb_params['gradientboostingclassifier__n_estimators'],max_depth=gb_params['gradientboostingclassifier__max_depth'],random_state=42))\n",
    "classifier=OOC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9633802816901409 0.9608500924641701 0.9620311123299864\n",
      "[CV 4/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 4/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.182, test=0.211) total time=  11.7s\n",
      "[CV 3/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 3/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.236, test=0.172) total time=   0.5s\n",
      "[CV 1/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 1/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.209, test=0.154) total time=  46.6s\n",
      "[CV 2/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 2/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.940) total time=   1.7s\n",
      "[CV 1/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.936) total time=  45.7s\n",
      "[CV 3/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[18:38:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.948) total time=  45.1s\n",
      "[CV 3/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[18:39:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.946) total time=  30.6s\n",
      "[CV 1/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 1/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.303, test=0.303) total time=   0.4s\n",
      "[CV 4/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 4/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.182, test=0.198) total time=  43.1s\n",
      "[CV 4/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 4/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.933) total time=   0.5s\n",
      "[CV 5/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 5/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.974) total time=   0.4s\n",
      "[CV 3/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 3/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.955) total time=   1.1s\n",
      "[CV 2/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.966) total time=  57.5s\n",
      "[CV 2/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[18:39:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.963) total time=  32.4s\n",
      "[CV 1/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[18:39:41] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.947) total time=  30.4s\n",
      "[CV 2/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 2/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.191, test=0.197) total time=   1.2s\n",
      "[CV 2/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 2/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.294, test=0.270) total time=   0.5s\n",
      "[CV 4/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 4/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.182, test=0.201) total time=  52.6s\n",
      "[CV 4/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 4/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.936) total time=   0.9s\n",
      "[CV 5/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 5/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.980) total time=   0.9s\n",
      "[CV 5/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[18:38:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.970) total time=  48.3s\n",
      "[CV 2/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[18:38:59] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.969) total time=  52.5s\n",
      "[CV 5/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[18:39:52] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.963) total time=  21.7s\n",
      "[CV 5/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 5/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.313, test=0.341) total time=   0.5s\n",
      "[CV 4/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 4/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.353, test=0.328) total time=   0.4s\n",
      "[CV 5/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 5/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.290, test=0.303) total time=   0.5s\n",
      "[CV 2/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 2/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.352, test=0.290) total time=   0.5s\n",
      "[CV 4/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 4/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.355, test=0.328) total time=   0.5s\n",
      "[CV 3/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 3/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.242, test=0.181) total time=   0.5s\n",
      "[CV 2/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 2/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.194, test=0.207) total time=   6.8s\n",
      "[CV 4/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 4/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.235, test=0.228) total time=   0.5s\n",
      "[CV 2/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 2/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.194, test=0.207) total time=   5.7s\n",
      "[CV 1/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 1/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.319, test=0.338) total time=   0.5s\n",
      "[CV 2/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 2/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.362, test=0.302) total time=   0.5s\n",
      "[CV 3/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 3/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.335, test=0.346) total time=   0.5s\n",
      "[CV 4/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 4/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.350, test=0.311) total time=   0.6s\n",
      "[CV 5/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 5/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.345, test=0.340) total time=   0.5s\n",
      "[CV 1/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 1/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.215, test=0.254) total time=   0.5s\n",
      "[CV 2/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 2/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.290, test=0.265) total time=   0.5s\n",
      "[CV 3/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 3/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.235, test=0.167) total time=   0.5s\n",
      "[CV 4/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 4/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.230, test=0.233) total time=   0.5s\n",
      "[CV 5/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 5/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.224, test=0.248) total time=   0.5s\n",
      "[CV 1/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 1/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.917) total time=   0.4s\n",
      "[CV 1/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 1/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.956) total time=   1.1s\n",
      "[CV 3/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[18:38:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.946) total time=  38.9s\n",
      "[CV 1/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[18:38:49] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.944) total time=  43.1s\n",
      "[CV 2/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[18:39:33] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.963) total time=  33.2s\n",
      "[CV 3/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 3/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.191, test=0.153) total time=   6.1s\n",
      "[CV 1/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 1/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.319, test=0.335) total time=   0.5s\n",
      "[CV 2/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 2/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.359, test=0.295) total time=   0.5s\n",
      "[CV 3/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 3/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.337, test=0.348) total time=   0.5s\n",
      "[CV 4/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 4/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.350, test=0.316) total time=   0.5s\n",
      "[CV 5/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 5/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.343, test=0.340) total time=   0.5s\n",
      "[CV 1/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 1/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.219, test=0.252) total time=   0.5s\n",
      "[CV 2/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 2/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.293, test=0.266) total time=   0.5s\n",
      "[CV 5/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 5/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.228, test=0.254) total time=   0.5s\n",
      "[CV 3/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 3/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.201, test=0.173) total time=  37.6s\n",
      "[CV 5/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 5/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.969) total time=   0.4s\n",
      "[CV 2/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 2/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.966) total time=   0.5s\n",
      "[CV 2/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 2/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.963) total time=   1.3s\n",
      "[CV 4/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.955) total time=  54.6s\n",
      "[CV 4/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[18:39:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.952) total time=  51.6s\n",
      "[CV 1/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 1/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.191, test=0.154) total time=   4.1s\n",
      "[CV 5/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 5/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.336, test=0.340) total time=   0.5s\n",
      "[CV 5/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 5/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.245, test=0.262) total time=   0.4s\n",
      "[CV 5/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 5/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.193, test=0.207) total time=  51.5s\n",
      "[CV 1/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 1/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.920) total time=   1.8s\n",
      "[CV 4/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.955) total time=  46.2s\n",
      "[CV 4/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[18:38:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.950) total time=  43.0s\n",
      "[CV 5/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[18:39:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.966) total time=  29.1s\n",
      "[CV 5/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 5/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.180, test=0.196) total time=  34.0s\n",
      "[CV 3/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 3/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.948) total time=   0.4s\n",
      "[CV 4/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 4/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.950) total time=   0.5s\n",
      "[CV 5/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 5/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.970) total time=   1.2s\n",
      "[CV 3/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.945) total time=  57.1s\n",
      "[CV 1/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[18:39:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.942) total time=  31.9s\n",
      "[CV 4/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[18:39:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.961) total time=  29.3s\n",
      "[CV 3/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 3/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.271, test=0.221) total time=   0.5s\n",
      "[CV 5/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 5/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.336, test=0.340) total time=   0.5s\n",
      "[CV 3/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 3/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.201, test=0.173) total time=  31.1s\n",
      "[CV 2/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 2/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.930) total time=   0.4s\n",
      "[CV 3/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 3/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.958) total time=   0.5s\n",
      "[CV 1/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 1/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.953) total time=   1.3s\n",
      "[CV 5/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.970) total time=  56.8s\n",
      "[CV 5/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[18:39:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.972) total time=  49.2s\n",
      "[CV 1/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 1/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.198, test=0.154) total time=  17.7s\n",
      "[CV 4/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 4/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.189, test=0.201) total time= 1.2min\n",
      "[CV 5/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 5/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.970) total time=   1.6s\n",
      "[CV 2/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.967) total time=  48.0s\n",
      "[CV 1/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[18:38:59] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.944) total time=  51.9s\n",
      "[CV 4/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[18:39:51] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.961) total time=  22.5s\n",
      "[CV 2/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 2/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.325, test=0.279) total time=   0.4s\n",
      "[CV 2/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 2/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.195, test=0.218) total time=   3.3s\n",
      "[CV 1/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 1/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.224, test=0.261) total time=   0.5s\n",
      "[CV 4/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 4/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.244, test=0.236) total time=   0.5s\n",
      "[CV 4/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 4/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.185, test=0.201) total time= 1.4min\n",
      "[CV 4/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 4/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.935) total time=   1.6s\n",
      "[CV 4/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[18:38:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.955) total time=  38.4s\n",
      "[CV 5/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[18:38:49] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.970) total time=  36.4s\n",
      "[CV 1/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[18:39:26] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.946) total time=  37.0s\n",
      "[CV 5/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 5/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.256, test=0.274) total time=   0.4s\n",
      "[CV 3/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 3/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.201, test=0.173) total time=  17.2s\n",
      "[CV 5/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 5/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.193, test=0.207) total time=  50.7s\n",
      "[CV 3/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 3/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.949) total time=   1.8s\n",
      "[CV 1/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.936) total time=  54.3s\n",
      "[CV 3/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[18:39:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.948) total time=  52.3s\n",
      "[CV 4/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 4/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.319, test=0.306) total time=   0.5s\n",
      "[CV 1/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 1/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.254, test=0.284) total time=   0.5s\n",
      "[CV 4/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 4/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.285, test=0.277) total time=   0.5s\n",
      "[CV 1/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 1/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.319, test=0.342) total time=   0.5s\n",
      "[CV 3/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 3/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.334, test=0.347) total time=   0.5s\n",
      "[CV 2/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 2/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.295, test=0.266) total time=   0.5s\n",
      "[CV 1/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 1/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.205, test=0.154) total time=  45.8s\n",
      "[CV 1/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 1/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.915) total time=   0.9s\n",
      "[CV 4/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 4/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.948) total time=   1.3s\n",
      "[CV 1/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.950) total time=  34.2s\n",
      "[CV 2/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[18:38:45] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.969) total time=  36.0s\n",
      "[CV 3/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[18:39:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.948) total time=  33.2s\n",
      "[CV 1/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 1/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.229, test=0.267) total time=   0.5s\n",
      "[CV 1/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 1/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.317, test=0.328) total time=   0.4s\n",
      "[CV 3/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 3/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.261, test=0.210) total time=   0.5s\n",
      "[CV 5/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 5/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.193, test=0.207) total time=  48.7s\n",
      "[CV 2/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 2/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.938) total time=   1.0s\n",
      "[CV 1/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[18:38:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.942) total time=  37.8s\n",
      "[CV 4/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[18:38:48] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.951) total time=  36.1s\n",
      "[CV 5/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[18:39:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.967) total time=  32.2s\n",
      "[CV 2/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 2/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.272, test=0.244) total time=   0.5s\n",
      "[CV 3/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 3/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.332, test=0.339) total time=   0.5s\n",
      "[CV 1/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 1/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.203, test=0.154) total time=  54.4s\n",
      "[CV 5/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 5/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.970) total time=   0.8s\n",
      "[CV 3/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 3/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.952) total time=   0.8s\n",
      "[CV 5/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.970) total time=  39.1s\n",
      "[CV 2/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[18:38:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.964) total time=  45.0s\n",
      "[CV 3/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[18:39:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.946) total time=  33.2s\n",
      "[CV 4/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 4/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.245, test=0.263) total time=   0.5s\n",
      "[CV 2/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 2/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.345, test=0.291) total time=   0.5s\n",
      "[CV 2/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 2/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.195, test=0.207) total time=   3.7s\n",
      "[CV 3/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 3/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.201, test=0.173) total time=  47.0s\n",
      "[CV 3/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 3/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.950) total time=   0.9s\n",
      "[CV 4/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 4/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.942) total time=   0.8s\n",
      "[CV 3/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[18:38:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.950) total time=  46.8s\n",
      "[CV 5/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[18:38:58] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.972) total time=  43.7s\n",
      "[CV 2/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[18:39:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.964) total time=  30.3s\n",
      "[CV 3/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 3/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.314, test=0.313) total time=   0.4s\n",
      "[CV 5/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 5/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.190, test=0.207) total time= 2.0min\n",
      "[CV 1/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 1/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.953) total time=   0.5s\n",
      "[CV 2/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 2/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.960) total time=   0.7s\n",
      "[CV 2/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[18:38:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.961) total time=  36.9s\n",
      "[CV 3/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[18:38:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.948) total time=  35.6s\n",
      "[CV 4/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[18:39:23] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.965) total time=  32.6s\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Progetto_R.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
