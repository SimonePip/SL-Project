{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_veeJ32BrfR",
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zxRpUUr-BnaJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Lk7tXbB_Vf",
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4TPaksx6CCw2"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_finale.csv', header ='infer').iloc[:,1:]\n",
    "#data's classes were initially (1,2,3,4) hence change to (0,1,2,3)\n",
    "data['class']=data['class']-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log(views)/log(com)']=(np.log(2+data['views']))/np.log((2+data['n_comments']))\n",
    "data['log(com)/log(views)']=(np.log(2+data['n_comments']))/np.log((2+data['views']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration- minutes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['durata']=round(data['durata']/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Titles elaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/simone/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    string=tokenizer.tokenize(string)\n",
    "    for word in range(len(string)):\n",
    "        string[word] = string[word].lower() \n",
    "    #Remove stopwords\n",
    "    string = [word for word in string if not word in stopwords.words()]\n",
    "    #STEMMING\n",
    "    stemmer = PorterStemmer()\n",
    "    string = [stemmer.stem(word) for word in string]\n",
    "    return \" \".join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13374it [04:24, 50.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#remove all the bad stuff we don't need\n",
    "titles=list(data['titolo'])\n",
    "for i,title in tqdm(enumerate(titles)):\n",
    "    titles[i]=clean(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of different words is:  30244\n",
      "the total number of words is:  76584\n"
     ]
    }
   ],
   "source": [
    "#create dictionary with key the word and value the number of times it has been seen\n",
    "diz={}\n",
    "for title in titles:\n",
    "    for word in title.split(' '):\n",
    "        if word in diz:\n",
    "            diz[word]+=1\n",
    "        else:\n",
    "            diz[word]=1  \n",
    "print('the total number of different words is: ',len(diz.keys()))\n",
    "print('the total number of words is: ',np.sum(list(diz.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update value of the dictionary with its prob computed as #(times is observed)/#(total words)\n",
    "tot=np.sum(list(diz.values()))\n",
    "for key in diz.keys():\n",
    "    diz[key]=diz[key]/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 13374/13374 [00:00<00:00, 283508.06it/s]\n"
     ]
    }
   ],
   "source": [
    "prob_title=[]\n",
    "for title in tqdm(titles):\n",
    "    somma=0\n",
    "    for word in title.split(' '):\n",
    "        somma+=(diz[word])\n",
    "    prob_title.append(somma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns = ['titolo'])\n",
    "data['titolo']=prob_title\n",
    "data['titolo']=(data['titolo']-data['titolo'].min())/(data['titolo'].max()-data['titolo'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SjeiTzLDz3J",
    "tags": []
   },
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk6Vu9B8FKUC"
   },
   "source": [
    "### Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zPpV61yRFRd2"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAXqkuGSRo6C",
    "outputId": "a43f65dd-42dd-4825-810a-8a25c90118cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['durata', 'views', 'n_comments', 'n_like', 'genere', 'subscribers',\n",
       "       'publ', 'max_quality', 'timedelta', 'score', 'class',\n",
       "       'log(views)/log(com)', 'log(com)/log(views)', 'titolo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Vh8Mxd3IFn3-"
   },
   "outputs": [],
   "source": [
    "Y = data['class'] # Extract the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TjwRGC1LF_yM"
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['class','score','publ','timedelta']) # Remove from the data useful to the analysis: \"score, publ, titolo, class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4kcZ6zbkM71k"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X['genere'] = le.fit_transform(X['genere']) # Transform the Categorical genere feature in a numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5xfQwZB6GN5C"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y, train_size=.8, random_state=42) # Split dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NnylF4nyHP05"
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train, train_size = .8, random_state = 42) # Split train in train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McL-rf4EIETM"
   },
   "source": [
    "### Excursus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfJ_pNuoIggO"
   },
   "source": [
    "We want to predict the class of a given video. The classes were defined using a home-made score.\n",
    "\n",
    "To predict we will try different models.\n",
    "Let's start with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sihcOc9uI77Y",
    "tags": []
   },
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Nf67Y6igJRo7"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxSP1WkcJzIn"
   },
   "source": [
    "REMINDER OF THE PARAMETERS\n",
    "\n",
    "\n",
    "C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOwe20T2JYWN"
   },
   "source": [
    "Try to use SVC without scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT1V1xqOMskK",
    "outputId": "d81d3c1e-2765-47f5-ea02-3ba82ae68a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167131569327672\n",
      "0.4897196261682243 0.2514992503748126\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma = 'auto', random_state = 42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbBrjFoJUAIv"
   },
   "source": [
    "Try to use SVC with Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOLK38YuUIO0",
    "outputId": "5e42c141-5d75-487d-cf12-62154784483d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4152634868298607\n",
      "0.5682242990654206 0.40886480885536736\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKoLo8RcUuK8"
   },
   "source": [
    "Try to use SVC with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6R5IQpLSU1f0",
    "outputId": "618acf6f-65ca-457b-a5fe-f0450fc43bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3065007325181236\n",
      "0.5532710280373832 0.34307846076961523\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4nb9eD-VBc_"
   },
   "source": [
    "Try to use SVC with MinMaxScaler and Standard Scaler (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-zC-ZjtVKW0",
    "outputId": "fb50c2aa-3bc6-4266-b11c-d5bc2b1c4250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4152634868298607\n",
      "0.5682242990654206 0.40886480885536736\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(),StandardScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSCZ8ep5VZmm"
   },
   "source": [
    "Try to use SVC with Standard Scaler and MinMaxScaler (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXpKohSrVhH_",
    "outputId": "b1cc0eb0-10ed-4ebd-e7be-08217907a407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3065007325181236\n",
      "0.5532710280373832 0.34307846076961523\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),MinMaxScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JzDW9SRVpeJ"
   },
   "source": [
    "Same Results as One Scaler.\n",
    "\n",
    "\n",
    "Best Results without Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUL7jdepWgXj",
    "outputId": "8b1e9ee6-3af7-487f-f941-0437878d52a7"
   },
   "outputs": [],
   "source": [
    "#svc = SVC()\n",
    "#\n",
    "##def gridsearch\n",
    "#params = {\n",
    "#    'kernel' : ('poly', 'rbf', 'sigmoid'),\n",
    "#    'C' : np.linspace(1, 100, num=5), \n",
    "#    #'degree' : [3,5,8],\n",
    "#    #'gamma' : ('auto','scale')\n",
    "#    \n",
    "#}\n",
    "#\n",
    "#search = GridSearchCV(svc,param_grid=params,scoring='f1_macro',\n",
    "#                                  n_jobs=-1, refit=True, verbose=10, pre_dispatch='10*n_jobs',\n",
    "#                                  return_train_score=True)\n",
    "#search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5rYVhLC7aiLn"
   },
   "outputs": [],
   "source": [
    "#best_one=search.best_estimator_\n",
    "#y_pred = best_one.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UUMPjA_rawaJ"
   },
   "outputs": [],
   "source": [
    "#f1_score(y_val, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGUfbcicdG2b",
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "w9jsJjJCczXZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score using Random Forest without scaling data:  0.6341116494268896\n",
      "0.677570093457944 0.6125594601734696\n"
     ]
    }
   ],
   "source": [
    "clf = RFC(random_state = 42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using Random Forest without scaling data: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "mKM5dKSUc9lt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6323011282860391\n",
      "0.6719626168224299 0.6144606772864322\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), RFC(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mkR3VGIcdATZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6290272219642454\n",
      "0.677570093457944 0.6053535822504661\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), RFC(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SSoHJ33mdEgY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier(n_jobs=-1,\n",
       "                                                               random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'randomforestclassifier__criterion': ['gini',\n",
       "                                                               'entropy'],\n",
       "                         'randomforestclassifier__max_features': [None],\n",
       "                         'randomforestclassifier__n_estimators': [50, 100,\n",
       "                                                                  200]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), RFC(random_state = 42, n_jobs = -1))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'randomforestclassifier__criterion' : ['gini','entropy'],\n",
    "          'randomforestclassifier__max_features' : [None],\n",
    "          'randomforestclassifier__n_estimators' : [50,100,200]\n",
    "         }\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6785046728971963 0.6211939095250446\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ghV7IJqudY3X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6370255990039098"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.37530499008824464\n",
      "PRECISION: 0.449174794397171\n",
      "RECALL: 0.36641039938138226\n",
      "ACCURACY: 0.5504672897196262\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=42, multi_class = 'multinomial',\n",
    "                                                         solver = 'newton-cg',\n",
    "                                                         n_jobs = -1))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall_fscore_support(y_val, y_pred, average='macro')[0])\n",
    "print('RECALL:',precision_recall_fscore_support(y_val, y_pred, average='macro')[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.44377398370086324\n",
      "PRECISION: 0.5451408831045041\n",
      "RECALL: 0.42072701443130156\n",
      "ACCURACY: 0.5514018691588785\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),KNeighborsClassifier(n_jobs = -1) )\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall_fscore_support(y_val, y_pred, average='macro')[0])\n",
    "print('RECALL:',precision_recall_fscore_support(y_val, y_pred, average='macro')[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.5445718983668641\n",
      "PRECISION: 0.5523157838773175\n",
      "RECALL: 0.5381562036944156\n",
      "ACCURACY: 0.5934579439252337\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),DecisionTreeClassifier(random_state = 42) )\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall_fscore_support(y_val, y_pred, average='macro')[0])\n",
    "print('RECALL:',precision_recall_fscore_support(y_val, y_pred, average='macro')[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGUfbcicdG2b",
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "w9jsJjJCczXZ"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:49:01] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost without scaling data:  0.6371253033381947\n",
      "0.6780373831775701 0.6244349246413563\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(random_state=42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost without scaling data: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "mKM5dKSUc9lt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:49:02] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost using MinMaxScaler:  0.6375949578127305\n",
      "0.6785046728971963 0.6248097372350594\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), XGBClassifier(random_state=42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost using MinMaxScaler: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "mkR3VGIcdATZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:49:03] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost using StandardScaler:  0.6341134136963655\n",
      "0.6771028037383178 0.6200598849158905\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), XGBClassifier(random_state=42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost using StandardScaler: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "SSoHJ33mdEgY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:59:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                       ('xgbclassifier',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      importance_type=None,\n",
       "                                                      interaction_constraints=None,\n",
       "                                                      learning_rate=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,...\n",
       "                                                      random_state=42,\n",
       "                                                      reg_alpha=None,\n",
       "                                                      reg_lambda=None,\n",
       "                                                      scale_pos_weight=None,\n",
       "                                                      subsample=None,\n",
       "                                                      tree_method=None,\n",
       "                                                      use_label_encoder=False,\n",
       "                                                      validate_parameters=None,\n",
       "                                                      verbosity=None))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgbclassifier__learning_rate': [0.2, 0.3, 0.4],\n",
       "                         'xgbclassifier__n_estimators': [600, 800, 1000]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), XGBClassifier(use_label_encoder=False,random_state=42))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'xgbclassifier__learning_rate' : [.2,.3,.4],\n",
    "          'xgbclassifier__n_estimators' : [600,800,1000]\n",
    "         }\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgbclassifier__learning_rate': 0.3, 'xgbclassifier__n_estimators': 600}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params=search.best_params_\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[455, 203,   9,   0],\n",
       "       [192, 755,  94,   3],\n",
       "       [  6, 123, 199,  22],\n",
       "       [  1,   9,  29,  40]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6677570093457944 0.6096385396813347 0.6243253215664104\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs Rest Classifier - ORCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier as ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:00:02] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:04] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                              ('xgbclassifier',\n",
       "                                               XGBClassifier(base_score=None,\n",
       "                                                             booster=None,\n",
       "                                                             colsample_bylevel=None,\n",
       "                                                             colsample_bynode=None,\n",
       "                                                             colsample_bytree=None,\n",
       "                                                             enable_categorical=False,\n",
       "                                                             gamma=None,\n",
       "                                                             gpu_id=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.3,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=None,\n",
       "                                                             min_child_weight=None,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             n_estimators=600,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             predictor=None,\n",
       "                                                             random_state=42,\n",
       "                                                             reg_alpha=None,\n",
       "                                                             reg_lambda=None,\n",
       "                                                             scale_pos_weight=None,\n",
       "                                                             subsample=None,\n",
       "                                                             tree_method=None,\n",
       "                                                             validate_parameters=None,\n",
       "                                                             verbosity=None))]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(MinMaxScaler(), XGBClassifier(learning_rate=xgb_params['xgbclassifier__learning_rate'],n_estimators=xgb_params['xgbclassifier__n_estimators'],random_state=42))\n",
    "classifier=ORC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6696261682242991 0.6159125703875006 0.6308186703437165\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs One Classifier - ORCO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier as OOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:00:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:11] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:12] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:13] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                             ('xgbclassifier',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_constraints=None,\n",
       "                                                            learning_rate=0.3,\n",
       "                                                            max_delta_step=None,\n",
       "                                                            max_depth=None,\n",
       "                                                            min_child_weight=None,\n",
       "                                                            missing=nan,\n",
       "                                                            monotone_constraints=None,\n",
       "                                                            n_estimators=600,\n",
       "                                                            n_jobs=None,\n",
       "                                                            num_parallel_tree=None,\n",
       "                                                            predictor=None,\n",
       "                                                            random_state=42,\n",
       "                                                            reg_alpha=None,\n",
       "                                                            reg_lambda=None,\n",
       "                                                            scale_pos_weight=None,\n",
       "                                                            subsample=None,\n",
       "                                                            tree_method=None,\n",
       "                                                            validate_parameters=None,\n",
       "                                                            verbosity=None))]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(MinMaxScaler(), XGBClassifier(learning_rate=xgb_params['xgbclassifier__learning_rate'],n_estimators=xgb_params['xgbclassifier__n_estimators'],random_state=42))\n",
    "classifier=OOC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6677570093457944 0.6128365323064808 0.6235577443586411\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.6558215690265023\n",
      "PRECISION: 0.6860272016066233\n",
      "RECALL: 0.6337452446169922\n",
      "ACCURACY: 0.6962616822429907\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "precision_recall = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall[0])\n",
    "print('RECALL:',precision_recall[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SCORE:  0.6483922876166766\n",
      "PRECISION: 0.6633200839334152\n",
      "RECALL: 0.6358695049702394\n",
      "ACCURACY: 0.6873831775700935\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), GradientBoostingClassifier(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "precision_recall = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
    "print('F1 SCORE: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print('PRECISION:',precision_recall[0])\n",
    "print('RECALL:',precision_recall[1])\n",
    "print('ACCURACY:', sum(y_pred == y_val)/len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "SSoHJ33mdEgY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('gradientboostingclassifier',\n",
       "                                        GradientBoostingClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'gradientboostingclassifier__learning_rate': [0.25,\n",
       "                                                                       0.3,\n",
       "                                                                       0.35],\n",
       "                         'gradientboostingclassifier__max_depth': [3, 4, 5],\n",
       "                         'gradientboostingclassifier__n_estimators': [70, 75,\n",
       "                                                                      80]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state = 42))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'gradientboostingclassifier__learning_rate' : [.25,.3,.35],#.285\n",
    "          'gradientboostingclassifier__n_estimators' : [70,75,80],\n",
    "          #'gradientboostingclassifier__n_iter_no_change':[2,5,10],\n",
    "          'gradientboostingclassifier__max_depth':[3,4,5]\n",
    "         }\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=1, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingclassifier__learning_rate': 0.25,\n",
       " 'gradientboostingclassifier__max_depth': 3,\n",
       " 'gradientboostingclassifier__n_estimators': 70}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_params=search.best_params_\n",
    "gb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[447, 209,  10,   1],\n",
       "       [175, 776,  91,   2],\n",
       "       [  4, 119, 205,  22],\n",
       "       [  0,   4,  32,  43]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6878504672897197 0.6301900067145451 0.6442553895245835\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs Rest Classifier (GB) - ORCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=Pipeline(steps=[('standardscaler',\n",
       "                                               StandardScaler()),\n",
       "                                              ('gradientboostingclassifier',\n",
       "                                               GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                                          n_estimators=70,\n",
       "                                                                          random_state=42))]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), GradientBoostingClassifier(learning_rate=gb_params['gradientboostingclassifier__learning_rate'],n_estimators=gb_params['gradientboostingclassifier__n_estimators'],max_depth=gb_params['gradientboostingclassifier__max_depth'],random_state=42))\n",
    "classifier=ORC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911214953271028 0.6221435288986007 0.6456388047178951\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs One Classifier (GB) - ORCO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=Pipeline(steps=[('standardscaler',\n",
       "                                              StandardScaler()),\n",
       "                                             ('gradientboostingclassifier',\n",
       "                                              GradientBoostingClassifier(learning_rate=0.25,\n",
       "                                                                         n_estimators=70,\n",
       "                                                                         random_state=42))]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), GradientBoostingClassifier(learning_rate=gb_params['gradientboostingclassifier__learning_rate'],n_estimators=gb_params['gradientboostingclassifier__n_estimators'],max_depth=gb_params['gradientboostingclassifier__max_depth'],random_state=42))\n",
    "classifier=OOC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6948598130841122 0.6459549550119454 0.6646510085722817\n",
      "[CV 5/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[17:51:30] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.602) total time= 3.4min\n",
      "[CV 2/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[17:54:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.613) total time= 3.5min\n",
      "[CV 5/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[17:58:27] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.593) total time= 1.5min\n",
      "[CV 1/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.619) total time= 3.3min\n",
      "[CV 3/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[17:54:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.598) total time= 2.9min\n",
      "[CV 4/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[17:57:45] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.644) total time= 1.9min\n",
      "[CV 4/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[17:51:30] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.646) total time= 4.1min\n",
      "[CV 2/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[17:55:38] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.608) total time= 2.2min\n",
      "[CV 2/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[17:57:48] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.605) total time= 2.1min\n",
      "[CV 4/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.651) total time= 2.7min\n",
      "[CV 5/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[17:54:12] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.605) total time= 2.3min\n",
      "[CV 1/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[17:56:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.608) total time= 2.6min\n",
      "[CV 1/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[17:51:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.623) total time= 4.1min\n",
      "[CV 1/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[17:55:37] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.610) total time= 2.1min\n",
      "[CV 5/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[17:57:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.594) total time= 1.9min\n",
      "[CV 1/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.610) total time= 2.6min\n",
      "[CV 2/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[17:54:04] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.615) total time= 2.3min\n",
      "[CV 3/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[17:56:22] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.609) total time= 2.1min\n",
      "[CV 4/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.646) total time= 3.4min\n",
      "[CV 1/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[17:54:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.605) total time= 3.5min\n",
      "[CV 4/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[17:58:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.640) total time= 1.5min\n",
      "[CV 2/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[17:51:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.597) total time= 4.1min\n",
      "[CV 5/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[17:55:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.597) total time= 3.4min\n",
      "[CV 3/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.589) total time= 2.7min\n",
      "[CV 4/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[17:54:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.646) total time= 2.3min\n",
      "[CV 5/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[17:56:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.594) total time= 2.2min\n",
      "[CV 3/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.585) total time= 3.4min\n",
      "[CV 5/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[17:54:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.601) total time= 2.9min\n",
      "[CV 1/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[17:57:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.613) total time= 2.1min\n",
      "[CV 2/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.599) total time= 2.8min\n",
      "[CV 2/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[17:54:15] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.609) total time= 2.9min\n",
      "[CV 3/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[17:57:12] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.604) total time= 2.2min\n",
      "[CV 1/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.621) total time= 2.6min\n",
      "[CV 3/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[17:54:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.596) total time= 2.3min\n",
      "[CV 4/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[17:56:23] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.645) total time= 2.1min\n",
      "[CV 3/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[17:51:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.582) total time= 4.0min\n",
      "[CV 4/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[17:55:33] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.639) total time= 3.4min\n",
      "[CV 2/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.596) total time= 3.4min\n",
      "[CV 4/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[17:54:51] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.642) total time= 2.9min\n",
      "[CV 3/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[17:57:49] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.604) total time= 2.0min\n",
      "[CV 5/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.598) total time= 4.0min\n",
      "[CV 3/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[17:55:32] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.599) total time= 3.4min\n",
      "[CV 5/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[17:51:28] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.609) total time= 2.7min\n",
      "[CV 1/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[17:54:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.609) total time= 2.9min\n",
      "[CV 2/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[17:57:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.607) total time= 2.3min\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Progetto_R.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
