{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_veeJ32BrfR"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "zxRpUUr-BnaJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Lk7tXbB_Vf"
   },
   "source": [
    "# Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "4TPaksx6CCw2"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_completo', header ='infer').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SjeiTzLDz3J"
   },
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk6Vu9B8FKUC"
   },
   "source": [
    "### Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "zPpV61yRFRd2"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAXqkuGSRo6C",
    "outputId": "a43f65dd-42dd-4825-810a-8a25c90118cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['titolo', 'durata', 'views', 'n_comments', 'n_like', 'genere', 'publ',\n",
       "       'max_quality', 'score', 'timedelta', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Vh8Mxd3IFn3-"
   },
   "outputs": [],
   "source": [
    "Y = data['class'] # Extract the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "TjwRGC1LF_yM"
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['class','score','publ','titolo']) # Remove from the data useful to the analysis: \"score, publ, titolo, class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "4kcZ6zbkM71k"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X['genere'] = le.fit_transform(X['genere']) # Transform the Categorical genere feature in a numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "5xfQwZB6GN5C"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y, train_size=.8, random_state=42) # Split dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "NnylF4nyHP05"
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train, train_size = .8, random_state = 42) # Split train in train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McL-rf4EIETM"
   },
   "source": [
    "### Excursus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfJ_pNuoIggO"
   },
   "source": [
    "We want to predict the class of a given video. The classes were defined using a home-made score.\n",
    "\n",
    "To predict we will try different models.\n",
    "Let's start with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sihcOc9uI77Y",
    "tags": []
   },
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "Nf67Y6igJRo7"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxSP1WkcJzIn"
   },
   "source": [
    "REMINDER OF THE PARAMETERS\n",
    "\n",
    "\n",
    "C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOwe20T2JYWN"
   },
   "source": [
    "Try to use SVC without scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT1V1xqOMskK",
    "outputId": "d81d3c1e-2765-47f5-ea02-3ba82ae68a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16061185468451242\n",
      "0.4732394366197183 0.25\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma = 'auto', random_state = 42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbBrjFoJUAIv"
   },
   "source": [
    "Try to use SVC with Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOLK38YuUIO0",
    "outputId": "5e42c141-5d75-487d-cf12-62154784483d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8075326343334872\n",
      "0.8267605633802817 0.803460182616736\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKoLo8RcUuK8"
   },
   "source": [
    "Try to use SVC with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6R5IQpLSU1f0",
    "outputId": "618acf6f-65ca-457b-a5fe-f0450fc43bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5932135459307548\n",
      "0.7422535211267606 0.5745550161812297\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4nb9eD-VBc_"
   },
   "source": [
    "Try to use SVC with MinMaxScaler and Standard Scaler (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-zC-ZjtVKW0",
    "outputId": "fb50c2aa-3bc6-4266-b11c-d5bc2b1c4250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8075326343334872\n",
      "0.8267605633802817 0.803460182616736\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(),StandardScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSCZ8ep5VZmm"
   },
   "source": [
    "Try to use SVC with Standard Scaler and MinMaxScaler (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXpKohSrVhH_",
    "outputId": "b1cc0eb0-10ed-4ebd-e7be-08217907a407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5932135459307548\n",
      "0.7422535211267606 0.5745550161812297\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),MinMaxScaler(), SVC(gamma='auto',random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JzDW9SRVpeJ"
   },
   "source": [
    "Same Results as One Scaler.\n",
    "\n",
    "\n",
    "Best Results without Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUL7jdepWgXj",
    "outputId": "8b1e9ee6-3af7-487f-f941-0437878d52a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': array([  1.  ,  25.75,  50.5 ,  75.25, 100.  ]),\n",
       "                         'kernel': ('poly', 'rbf', 'sigmoid')},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "\n",
    "#def gridsearch\n",
    "params = {\n",
    "    'kernel' : ('poly', 'rbf', 'sigmoid'),\n",
    "    'C' : np.linspace(1, 100, num=5), \n",
    "    #'degree' : [3,5,8],\n",
    "    #'gamma' : ('auto','scale')\n",
    "    \n",
    "}\n",
    "\n",
    "search = GridSearchCV(svc,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1, refit=True, verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "5rYVhLC7aiLn"
   },
   "outputs": [],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "UUMPjA_rawaJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31184183717669406"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGUfbcicdG2b",
    "tags": []
   },
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "w9jsJjJCczXZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score using Random Forest without scaling data:  0.9425383988190441\n",
      "0.9436619718309859 0.9377152681460934\n"
     ]
    }
   ],
   "source": [
    "clf = RFC(random_state = 42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using Random Forest without scaling data: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "mKM5dKSUc9lt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9462186951230818\n",
      "0.9436619718309859 0.9366736014794268\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), RFC(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "mkR3VGIcdATZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9425383988190441\n",
      "0.9436619718309859 0.9377152681460934\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), RFC(random_state = 42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "SSoHJ33mdEgY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier(n_jobs=-1,\n",
       "                                                               random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'randomforestclassifier__criterion': ['gini',\n",
       "                                                               'entropy'],\n",
       "                         'randomforestclassifier__max_features': [None],\n",
       "                         'randomforestclassifier__n_estimators': [50, 100,\n",
       "                                                                  200]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), RFC(random_state = 42, n_jobs = -1))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'randomforestclassifier__criterion' : ['gini','entropy'],\n",
    "          'randomforestclassifier__max_features' : [None],\n",
    "          'randomforestclassifier__n_estimators' : [50,100,200]\n",
    "          \n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9464788732394366 0.9490016759130837\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "ghV7IJqudY3X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497116153062941"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGUfbcicdG2b",
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "w9jsJjJCczXZ"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "OwUC3P8tc41w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:43:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost without scaling data:  0.9584295743338952\n",
      "0.9577464788732394 0.9579764794267223\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(random_state=42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost without scaling data: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "mKM5dKSUc9lt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:43:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost using MinMaxScaler:  0.9584295743338952\n",
      "0.9577464788732394 0.9579764794267223\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), XGBClassifier(random_state=42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost using MinMaxScaler: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "mkR3VGIcdATZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:43:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1-score using XGBoost using StandardScaler:  0.9584295743338952\n",
      "0.9577464788732394 0.9579764794267223\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), XGBClassifier(random_state=42))\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('f1-score using XGBoost using StandardScaler: ',f1_score(y_val, y_pred, average = 'macro'))\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "SSoHJ33mdEgY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/simone/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:15] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                       ('xgbclassifier',\n",
       "                                        XGBClassifier(base_score=None,\n",
       "                                                      booster=None,\n",
       "                                                      colsample_bylevel=None,\n",
       "                                                      colsample_bynode=None,\n",
       "                                                      colsample_bytree=None,\n",
       "                                                      enable_categorical=False,\n",
       "                                                      gamma=None, gpu_id=None,\n",
       "                                                      importance_type=None,\n",
       "                                                      interaction_constraints=None,\n",
       "                                                      learning_rate=None,\n",
       "                                                      max_delta_step=None,\n",
       "                                                      max_depth=None,...\n",
       "                                                      predictor=None,\n",
       "                                                      random_state=42,\n",
       "                                                      reg_alpha=None,\n",
       "                                                      reg_lambda=None,\n",
       "                                                      scale_pos_weight=None,\n",
       "                                                      subsample=None,\n",
       "                                                      tree_method=None,\n",
       "                                                      validate_parameters=None,\n",
       "                                                      verbosity=None))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'xgbclassifier__learning_rate': [0.2, 0.3, 0.4],\n",
       "                         'xgbclassifier__n_estimators': [600, 800, 1000]},\n",
       "             pre_dispatch='10*n_jobs', return_train_score=True,\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), XGBClassifier(random_state=42))\n",
    "\n",
    "#def gridsearch\n",
    "params = {'xgbclassifier__learning_rate' : [.2,.3,.4],\n",
    "          'xgbclassifier__n_estimators' : [600,800,1000]\n",
    "         }\n",
    "\n",
    "search = GridSearchCV(clf,param_grid=params,scoring='f1_macro',\n",
    "                                  n_jobs=-1,refit=True,verbose=10, pre_dispatch='10*n_jobs',\n",
    "                                  return_train_score=True)\n",
    "search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgbclassifier__learning_rate': 0.3, 'xgbclassifier__n_estimators': 1000}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params=search.best_params_\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[197,   9,   0,   0],\n",
       "       [  7, 323,   6,   0],\n",
       "       [  0,   6, 133,   1],\n",
       "       [  0,   0,   1,  27]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "0Xb7hb4jdUF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619718309859155 0.9622919556171984 0.9619776509111083\n"
     ]
    }
   ],
   "source": [
    "best_one=search.best_estimator_\n",
    "y_pred = best_one.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs Rest Classifier - ORCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier as ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                              ('xgbclassifier',\n",
       "                                               XGBClassifier(base_score=None,\n",
       "                                                             booster=None,\n",
       "                                                             colsample_bylevel=None,\n",
       "                                                             colsample_bynode=None,\n",
       "                                                             colsample_bytree=None,\n",
       "                                                             enable_categorical=False,\n",
       "                                                             gamma=None,\n",
       "                                                             gpu_id=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.3,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=None,\n",
       "                                                             min_child_weight=None,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             n_estimators=1000,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             predictor=None,\n",
       "                                                             random_state=42,\n",
       "                                                             reg_alpha=None,\n",
       "                                                             reg_lambda=None,\n",
       "                                                             scale_pos_weight=None,\n",
       "                                                             subsample=None,\n",
       "                                                             tree_method=None,\n",
       "                                                             validate_parameters=None,\n",
       "                                                             verbosity=None))]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(MinMaxScaler(), XGBClassifier(learning_rate=xgb_params['xgbclassifier__learning_rate'],n_estimators=xgb_params['xgbclassifier__n_estimators'],random_state=42))\n",
    "classifier=ORC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9661971830985916 0.9654631877022655 0.9694869948139254\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One vs One Classifier - ORCO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier as OOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:18] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:18] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:18] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:19] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:19] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:19] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                             ('xgbclassifier',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_constraints=None,\n",
       "                                                            learning_rate=0.3,\n",
       "                                                            max_delta_step=None,\n",
       "                                                            max_depth=None,\n",
       "                                                            min_child_weight=None,\n",
       "                                                            missing=nan,\n",
       "                                                            monotone_constraints=None,\n",
       "                                                            n_estimators=1000,\n",
       "                                                            n_jobs=None,\n",
       "                                                            num_parallel_tree=None,\n",
       "                                                            predictor=None,\n",
       "                                                            random_state=42,\n",
       "                                                            reg_alpha=None,\n",
       "                                                            reg_lambda=None,\n",
       "                                                            scale_pos_weight=None,\n",
       "                                                            subsample=None,\n",
       "                                                            tree_method=None,\n",
       "                                                            validate_parameters=None,\n",
       "                                                            verbosity=None))]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=make_pipeline(MinMaxScaler(), XGBClassifier(learning_rate=xgb_params['xgbclassifier__learning_rate'],n_estimators=xgb_params['xgbclassifier__n_estimators'],random_state=42))\n",
    "classifier=OOC(clf)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9647887323943662 0.9703507859454461 0.9682739122429602\n",
      "[CV 5/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 5/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.313, test=0.341) total time=   0.5s\n",
      "[CV 3/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 3/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.332, test=0.339) total time=   0.5s\n",
      "[CV 1/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 1/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.203, test=0.154) total time=  22.0s\n",
      "[CV 4/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 4/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.933) total time=   0.4s\n",
      "[CV 1/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 1/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.942) total time=   1.0s\n",
      "[CV 2/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.977) total time=  38.5s\n",
      "[CV 3/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[13:43:44] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.941) total time=  36.8s\n",
      "[CV 3/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[13:44:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.942) total time=  34.8s\n",
      "[CV 4/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 4/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.245, test=0.263) total time=   0.5s\n",
      "[CV 1/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 1/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.254, test=0.284) total time=   0.5s\n",
      "[CV 3/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 3/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.201, test=0.173) total time=  15.2s\n",
      "[CV 4/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 4/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.350, test=0.311) total time=   0.6s\n",
      "[CV 1/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 1/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.215, test=0.254) total time=   0.5s\n",
      "[CV 3/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 3/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.235, test=0.167) total time=   0.5s\n",
      "[CV 5/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 5/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.224, test=0.248) total time=   0.5s\n",
      "[CV 2/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 2/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.934) total time=   0.4s\n",
      "[CV 2/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 2/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.967) total time=   0.5s\n",
      "[CV 5/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 5/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.979) total time=   0.8s\n",
      "[CV 2/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.972) total time=  53.8s\n",
      "[CV 2/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[13:44:00] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.968) total time=  56.1s\n",
      "[CV 4/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 4/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.182, test=0.211) total time=   5.3s\n",
      "[CV 4/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 4/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.182, test=0.201) total time= 1.6min\n",
      "[CV 5/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 5/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.968) total time=   1.6s\n",
      "[CV 4/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.952) total time=  39.8s\n",
      "[CV 4/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[13:43:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.959) total time=  39.5s\n",
      "[CV 5/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[13:44:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.966) total time=  33.2s\n",
      "[CV 3/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 3/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.191, test=0.153) total time=   4.5s\n",
      "[CV 3/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 3/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.242, test=0.181) total time=   0.5s\n",
      "[CV 3/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 3/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.201, test=0.173) total time=  26.1s\n",
      "[CV 2/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 2/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.933) total time=   0.9s\n",
      "[CV 2/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 2/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.968) total time=   1.1s\n",
      "[CV 2/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.977) total time= 1.0min\n",
      "[CV 3/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[13:44:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.944) total time=  53.2s\n",
      "[CV 4/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 4/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.319, test=0.306) total time=   0.5s\n",
      "[CV 4/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 4/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.353, test=0.328) total time=   0.5s\n",
      "[CV 2/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 2/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.195, test=0.207) total time=   4.4s\n",
      "[CV 2/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 2/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.359, test=0.295) total time=   0.5s\n",
      "[CV 4/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 4/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.350, test=0.316) total time=   0.5s\n",
      "[CV 1/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 1/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.219, test=0.253) total time=   0.5s\n",
      "[CV 3/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 3/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.238, test=0.172) total time=   0.5s\n",
      "[CV 5/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 5/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.228, test=0.254) total time=   0.5s\n",
      "[CV 3/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 3/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.201, test=0.173) total time=  19.9s\n",
      "[CV 1/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 1/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.932) total time=   1.0s\n",
      "[CV 4/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 4/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.948) total time=   1.1s\n",
      "[CV 3/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.947) total time= 1.0min\n",
      "[CV 4/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[13:44:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.961) total time=  53.7s\n",
      "[CV 2/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 2/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.272, test=0.244) total time=   0.5s\n",
      "[CV 2/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 2/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.345, test=0.291) total time=   0.5s\n",
      "[CV 4/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 4/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.285, test=0.277) total time=   0.5s\n",
      "[CV 5/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 5/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.190, test=0.207) total time=  46.8s\n",
      "[CV 5/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 5/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.970) total time=   0.9s\n",
      "[CV 4/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 4/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.942) total time=   0.9s\n",
      "[CV 1/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.944) total time= 1.0min\n",
      "[CV 5/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[13:44:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.976) total time=  53.4s\n",
      "[CV 3/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 3/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.271, test=0.221) total time=   0.5s\n",
      "[CV 5/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 5/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.336, test=0.340) total time=   0.5s\n",
      "[CV 5/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 5/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.290, test=0.303) total time=   0.5s\n",
      "[CV 2/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 2/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.352, test=0.290) total time=   0.5s\n",
      "[CV 4/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 4/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.355, test=0.328) total time=   0.5s\n",
      "[CV 2/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 2/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.295, test=0.266) total time=   0.5s\n",
      "[CV 1/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 1/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.203, test=0.154) total time=  25.1s\n",
      "[CV 5/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 5/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.965) total time=   0.4s\n",
      "[CV 4/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 4/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.949) total time=   0.6s\n",
      "[CV 1/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.944) total time=  40.4s\n",
      "[CV 5/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[13:43:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.976) total time=  40.1s\n",
      "[CV 1/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[13:44:27] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.946) total time=  36.4s\n",
      "[CV 1/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 1/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.303, test=0.303) total time=   0.4s\n",
      "[CV 3/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 3/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.201, test=0.173) total time=  10.4s\n",
      "[CV 4/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 4/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.187, test=0.201) total time= 1.7min\n",
      "[CV 1/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 1/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.945) total time=   0.5s\n",
      "[CV 2/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 2/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.961) total time=   1.0s\n",
      "[CV 5/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.973) total time=  40.8s\n",
      "[CV 1/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[13:43:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.945) total time=  48.2s\n",
      "[CV 2/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[13:44:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.966) total time=  33.5s\n",
      "[CV 1/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 1/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.191, test=0.154) total time=   4.7s\n",
      "[CV 4/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 4/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.243, test=0.236) total time=   0.5s\n",
      "[CV 5/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 5/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.193, test=0.207) total time=  55.4s\n",
      "[CV 1/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 1/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.925) total time=   1.6s\n",
      "[CV 3/5; 1/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 1/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.947) total time=  42.3s\n",
      "[CV 2/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[13:43:48] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.968) total time=  48.7s\n",
      "[CV 3/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[13:44:37] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.950) total time=  32.0s\n",
      "[CV 1/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 1/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.229, test=0.267) total time=   0.5s\n",
      "[CV 5/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 5/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.190, test=0.207) total time= 1.1min\n",
      "[CV 3/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 3/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.954) total time=   1.7s\n",
      "[CV 1/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.944) total time=  49.0s\n",
      "[CV 4/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[13:43:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.959) total time=  43.4s\n",
      "[CV 4/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[13:44:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.954) total time=  31.0s\n",
      "[CV 5/5; 3/15] START C=1.0, kernel=sigmoid......................................\n",
      "[CV 5/5; 3/15] END C=1.0, kernel=sigmoid;, score=(train=0.256, test=0.274) total time=   0.5s\n",
      "[CV 1/5; 5/15] START C=25.75, kernel=rbf........................................\n",
      "[CV 1/5; 5/15] END C=25.75, kernel=rbf;, score=(train=0.317, test=0.328) total time=   0.4s\n",
      "[CV 3/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 3/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.261, test=0.210) total time=   0.5s\n",
      "[CV 1/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 1/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.319, test=0.342) total time=   0.5s\n",
      "[CV 3/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 3/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.334, test=0.347) total time=   0.5s\n",
      "[CV 1/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 1/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.224, test=0.261) total time=   0.5s\n",
      "[CV 2/5; 10/15] START C=75.25, kernel=poly......................................\n",
      "[CV 2/5; 10/15] END C=75.25, kernel=poly;, score=(train=0.194, test=0.207) total time=   5.2s\n",
      "[CV 2/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 2/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.194, test=0.207) total time=  11.5s\n",
      "[CV 3/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 3/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.949) total time=   0.4s\n",
      "[CV 3/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 3/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.955) total time=   0.5s\n",
      "[CV 3/5; 5/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 3/5; 5/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.951) total time=   0.8s\n",
      "[CV 4/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.951) total time=  48.0s\n",
      "[CV 3/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[13:43:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.944) total time=  46.7s\n",
      "[CV 5/5; 8/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800\n",
      "[13:44:41] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 8/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.966) total time=  30.0s\n",
      "[CV 2/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 2/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.325, test=0.279) total time=   0.4s\n",
      "[CV 4/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 4/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.182, test=0.198) total time=  43.2s\n",
      "[CV 4/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 4/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.934) total time=   1.0s\n",
      "[CV 3/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 3/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.960) total time=   1.1s\n",
      "[CV 4/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.951) total time= 1.0min\n",
      "[CV 2/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[13:44:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.964) total time=  34.5s\n",
      "[CV 2/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[13:44:44] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.961) total time=  29.1s\n",
      "[CV 3/5; 2/15] START C=1.0, kernel=rbf..........................................\n",
      "[CV 3/5; 2/15] END C=1.0, kernel=rbf;, score=(train=0.314, test=0.313) total time=   0.4s\n",
      "[CV 2/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 2/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.195, test=0.218) total time=   3.1s\n",
      "[CV 5/5; 8/15] START C=50.5, kernel=rbf.........................................\n",
      "[CV 5/5; 8/15] END C=50.5, kernel=rbf;, score=(train=0.336, test=0.340) total time=   0.5s\n",
      "[CV 5/5; 9/15] START C=50.5, kernel=sigmoid.....................................\n",
      "[CV 5/5; 9/15] END C=50.5, kernel=sigmoid;, score=(train=0.245, test=0.262) total time=   0.5s\n",
      "[CV 1/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 1/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.319, test=0.335) total time=   0.5s\n",
      "[CV 3/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 3/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.336, test=0.348) total time=   0.5s\n",
      "[CV 5/5; 11/15] START C=75.25, kernel=rbf.......................................\n",
      "[CV 5/5; 11/15] END C=75.25, kernel=rbf;, score=(train=0.343, test=0.340) total time=   0.5s\n",
      "[CV 2/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 2/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.293, test=0.266) total time=   0.5s\n",
      "[CV 4/5; 12/15] START C=75.25, kernel=sigmoid...................................\n",
      "[CV 4/5; 12/15] END C=75.25, kernel=sigmoid;, score=(train=0.235, test=0.228) total time=   0.5s\n",
      "[CV 1/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 1/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.206, test=0.154) total time=  32.3s\n",
      "[CV 3/5; 2/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100\n",
      "[CV 3/5; 2/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=100;, score=(train=1.000, test=0.953) total time=   0.9s\n",
      "[CV 1/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 1/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.948) total time=   1.2s\n",
      "[CV 5/5; 3/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 3/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.973) total time= 1.0min\n",
      "[CV 1/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[13:44:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.946) total time=  33.0s\n",
      "[CV 1/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[13:44:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.946) total time=  31.3s\n",
      "[CV 2/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 2/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.191, test=0.197) total time=   1.1s\n",
      "[CV 2/5; 6/15] START C=25.75, kernel=sigmoid....................................\n",
      "[CV 2/5; 6/15] END C=25.75, kernel=sigmoid;, score=(train=0.294, test=0.270) total time=   0.5s\n",
      "[CV 4/5; 7/15] START C=50.5, kernel=poly........................................\n",
      "[CV 4/5; 7/15] END C=50.5, kernel=poly;, score=(train=0.182, test=0.201) total time= 1.1min\n",
      "[CV 2/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 2/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.944) total time=   1.7s\n",
      "[CV 3/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.947) total time=  52.3s\n",
      "[CV 5/5; 5/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800\n",
      "[13:43:58] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 5/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.976) total time=  47.8s\n",
      "[CV 3/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[13:44:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.945) total time=  27.8s\n",
      "[CV 1/5; 4/15] START C=25.75, kernel=poly.......................................\n",
      "[CV 1/5; 4/15] END C=25.75, kernel=poly;, score=(train=0.198, test=0.154) total time=  12.7s\n",
      "[CV 5/5; 13/15] START C=100.0, kernel=poly......................................\n",
      "[CV 5/5; 13/15] END C=100.0, kernel=poly;, score=(train=0.193, test=0.207) total time= 1.1min\n",
      "[CV 4/5; 3/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 4/5; 3/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.936) total time=   1.8s\n",
      "[CV 5/5; 2/9] START xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 2/9] END xgbclassifier__learning_rate=0.2, xgbclassifier__n_estimators=800;, score=(train=1.000, test=0.973) total time=  52.4s\n",
      "[CV 1/5; 6/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000\n",
      "[13:43:59] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 6/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.945) total time=  54.8s\n",
      "[CV 4/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[13:44:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.953) total time=  21.0s\n",
      "[CV 5/5; 1/15] START C=1.0, kernel=poly.........................................\n",
      "[CV 5/5; 1/15] END C=1.0, kernel=poly;, score=(train=0.180, test=0.196) total time=  15.2s\n",
      "[CV 1/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 1/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.319, test=0.338) total time=   0.5s\n",
      "[CV 2/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 2/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.362, test=0.302) total time=   0.5s\n",
      "[CV 3/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 3/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.335, test=0.346) total time=   0.5s\n",
      "[CV 5/5; 14/15] START C=100.0, kernel=rbf.......................................\n",
      "[CV 5/5; 14/15] END C=100.0, kernel=rbf;, score=(train=0.345, test=0.340) total time=   0.5s\n",
      "[CV 2/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 2/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.290, test=0.265) total time=   0.5s\n",
      "[CV 4/5; 15/15] START C=100.0, kernel=sigmoid...................................\n",
      "[CV 4/5; 15/15] END C=100.0, kernel=sigmoid;, score=(train=0.229, test=0.233) total time=   0.5s\n",
      "[CV 1/5; 1/6] START randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 1/5; 1/6] END randomforestclassifier__criterion=gini, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.929) total time=   0.4s\n",
      "[CV 5/5; 4/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50\n",
      "[CV 5/5; 4/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=50;, score=(train=1.000, test=0.974) total time=   0.6s\n",
      "[CV 5/5; 6/6] START randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200\n",
      "[CV 5/5; 6/6] END randomforestclassifier__criterion=entropy, randomforestclassifier__max_features=None, randomforestclassifier__n_estimators=200;, score=(train=1.000, test=0.970) total time=   1.1s\n",
      "[CV 1/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[13:43:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.943) total time=  34.6s\n",
      "[CV 2/5; 4/9] START xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600\n",
      "[13:43:41] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 4/9] END xgbclassifier__learning_rate=0.3, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.968) total time=  40.3s\n",
      "[CV 4/5; 7/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600\n",
      "[13:44:22] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 7/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=600;, score=(train=1.000, test=0.957) total time=  32.2s\n",
      "[CV 5/5; 9/9] START xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000\n",
      "[13:44:54] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1645117766796/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 9/9] END xgbclassifier__learning_rate=0.4, xgbclassifier__n_estimators=1000;, score=(train=1.000, test=0.966) total time=  21.0s\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred), recall_score(y_val, y_pred, average='macro'), f1_score(y_val, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Progetto_R.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
